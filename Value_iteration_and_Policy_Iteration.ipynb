{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook includes implementations of the following algorithms:**\n",
        "\n",
        "\n",
        "*   Value Iteration Algorithm\n",
        "*   Policy Iteration Algorithm\n",
        "\n",
        "The notebook also includes a test gridworld game where the two algorithms are implemented to extract the optimal policy using value iteration and policy iteration.\n",
        "\n",
        "Below is the commented implementation where each section is in a seperate notebook.\n"
      ],
      "metadata": {
        "id": "47TabS3MJEXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "n_Pz4SvuNJYH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid world class representing the dynamics of the grid (enviroment) including the following:**\n",
        "\n",
        "\n",
        "*   Grid size\n",
        "*   Immediate rewards\n",
        "*   Possible actions\n",
        "*   States of the game which are indeed the cells in the grid\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Of6TdXscAsdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.actions = ['UP', 'DOWN', 'RIGHT', 'LEFT']\n",
        "        self.discount = 0.99\n",
        "        self.rewards = np.array([[0, -1, 10],\n",
        "                                  [-1, -1, -1],\n",
        "                                  [-1, -1, -1]])\n",
        "        self.transitions = {'UP': [0.8, 0.1, 0.1, 0],\n",
        "                            'DOWN': [0.1, 0.8, 0, 0.1],\n",
        "                            'RIGHT': [0.1, 0, 0.8, 0.1],\n",
        "                            'LEFT': [0.1, 0.1, 0, 0.8]}\n",
        "\n",
        "    def next_position(self, i, j, action):\n",
        "        if action == 'UP':\n",
        "            return max(i - 1, 0), j\n",
        "        elif action == 'DOWN':\n",
        "            return min(i + 1, self.size - 1), j\n",
        "        elif action == 'RIGHT':\n",
        "            return i, min(j + 1, self.size - 1)\n",
        "        elif action == 'LEFT':\n",
        "            return i, max(j - 1, 0)"
      ],
      "metadata": {
        "id": "lPCl8oWkkt-A"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section, the class agent algorithms is implemented including the following:**\n",
        "\n",
        "\n",
        "*   Value Iteration: finds the optimal value function for each state\n",
        "*   Policy Extraction: finds the optimal policy based on the output of the value iteration.\n",
        "*   Policy Iteration\n",
        "\n"
      ],
      "metadata": {
        "id": "bvHweWbjBnJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, world: GridWorld):\n",
        "        self.world = world\n",
        "\n",
        "    def value_iteration(self):\n",
        "        epsilon = 0.01\n",
        "        while True:\n",
        "            change = 0\n",
        "            for i in range(self.world.size):\n",
        "                for j in range(self.world.size):\n",
        "                    if self.world.rewards[i][j] in [self.world.rewards[0][0], 10]:\n",
        "                        continue\n",
        "                    old_value = self.world.value_function[i][j]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in self.world.actions:\n",
        "                        action_probs = self.world.transitions[action]\n",
        "                        action_value = 0\n",
        "                        for k, prob in enumerate(action_probs):\n",
        "                            new_i, new_j = self.world.next_position(i, j, self.world.actions[k])\n",
        "                            if self.world.rewards[new_i][new_j] in [-3, 10]:\n",
        "                                action_value += prob * self.world.rewards[new_i][new_j]\n",
        "                            else:\n",
        "                                action_value += prob * (self.world.rewards[new_i][new_j] +\n",
        "                                                         self.world.discount * self.world.value_function[new_i][new_j])\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                    self.world.value_function[i][j] = new_value\n",
        "                    change = max(change, abs(old_value - self.world.value_function[i][j]))\n",
        "            if change < epsilon:\n",
        "                break\n",
        "\n",
        "    def optimal_policy(self, reward):\n",
        "        self.world.rewards[0][0] = reward\n",
        "        self.world.value_function = np.zeros((self.world.size, self.world.size))\n",
        "        self.value_iteration()\n",
        "        policy = np.zeros((self.world.size, self.world.size), dtype=str)\n",
        "        for i in range(self.world.size):\n",
        "            for j in range(self.world.size):\n",
        "                if self.world.rewards[i][j] in [reward, 10]:\n",
        "                    continue\n",
        "                best_action = None\n",
        "                best_value = float('-inf')\n",
        "                for action in self.world.actions:\n",
        "                    action_probs = self.world.transitions[action]\n",
        "                    action_value = 0\n",
        "                    for k, prob in enumerate(action_probs):\n",
        "                        new_i, new_j = self.world.next_position(i, j, self.world.actions[k])\n",
        "                        if self.world.rewards[new_i][new_j] in [-3, 10]:\n",
        "                            action_value += prob * self.world.rewards[new_i][new_j]\n",
        "                        else:\n",
        "                            action_value += prob * (self.world.rewards[new_i][new_j] +\n",
        "                                                     self.world.discount * self.world.value_function[new_i][new_j])\n",
        "                    if action_value > best_value:\n",
        "                        best_value = action_value\n",
        "                        best_action = action\n",
        "                policy[i][j] = best_action\n",
        "        policy[0][0] = 'T'\n",
        "        policy[0][2] = 'T'\n",
        "        return self.world.value_function, policy"
      ],
      "metadata": {
        "id": "Ub0XCnVDQLtd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section**, The test cases are implemented using different values for the variable reward r and a discount factor = 0.99 (The future is highly accounted in the calculation)"
      ],
      "metadata": {
        "id": "ZIIhGQVZx8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridWorld(size=3)\n",
        "agent = Agent(grid)\n",
        "rewards = [100, 3, 0, -3]\n",
        "for r in rewards:\n",
        "    value_function, optimal_policy = agent.optimal_policy(r)\n",
        "    print(f\"r = {r}:\")\n",
        "    print(\"-------\")\n",
        "    print(\"Value Function (using Value Iteration):\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(value_function)\n",
        "    print(\"\\nPolicy (Optimal using Policy Iteration):\")\n",
        "    print(\"----------------------------------------\")\n",
        "    print(optimal_policy)\n",
        "    print(\"===============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OURKWnsEx9gq",
        "outputId": "11b9e3f5-8650-4a9f-f6f1-c4d7a1e7d1c3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = 100:\n",
            "-------\n",
            "Value Function (using Value Iteration):\n",
            "---------------------------------------\n",
            "[[ 0.         99.16163177  0.        ]\n",
            " [98.8595135  96.41608506 85.45794401]\n",
            " [96.11381005 93.96967881 90.88072671]]\n",
            "\n",
            "Policy (Optimal using Policy Iteration):\n",
            "----------------------------------------\n",
            "[['T' 'L' 'T']\n",
            " ['U' 'L' 'L']\n",
            " ['U' 'L' 'L']]\n",
            "===============================================================\n",
            "r = 3:\n",
            "-------\n",
            "Value Function (using Value Iteration):\n",
            "---------------------------------------\n",
            "[[0.         9.10099888 0.        ]\n",
            " [6.46104359 8.10949731 9.55692641]\n",
            " [5.67926929 6.91829271 8.19072475]]\n",
            "\n",
            "Policy (Optimal using Policy Iteration):\n",
            "----------------------------------------\n",
            "[['T' 'R' 'T']\n",
            " ['R' 'R' 'U']\n",
            " ['R' 'U' 'U']]\n",
            "===============================================================\n",
            "r = 0:\n",
            "-------\n",
            "Value Function (using Value Iteration):\n",
            "---------------------------------------\n",
            "[[0.         8.76803551 0.        ]\n",
            " [6.06459772 8.0372858  9.55692641]\n",
            " [5.57989863 6.85481643 8.19072475]]\n",
            "\n",
            "Policy (Optimal using Policy Iteration):\n",
            "----------------------------------------\n",
            "[['T' 'R' 'T']\n",
            " ['R' 'R' 'U']\n",
            " ['R' 'U' 'U']]\n",
            "===============================================================\n",
            "r = -3:\n",
            "-------\n",
            "Value Function (using Value Iteration):\n",
            "---------------------------------------\n",
            "[[0.         8.43507213 0.        ]\n",
            " [5.66815369 7.96507446 9.55692641]\n",
            " [5.50447969 6.82004202 8.19072475]]\n",
            "\n",
            "Policy (Optimal using Policy Iteration):\n",
            "----------------------------------------\n",
            "[['T' 'R' 'T']\n",
            " ['R' 'R' 'U']\n",
            " ['R' 'R' 'U']]\n",
            "===============================================================\n"
          ]
        }
      ]
    }
  ]
}