{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHsg2WeA2Y0F6+7D+CVy5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omaremad02/Markov-Decision-Process/blob/main/Value_iteration_and_Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook includes implementations of the following algorithms:**\n",
        "\n",
        "\n",
        "*   Value Iteration Algorithm\n",
        "*   Policy Iteration Algorithm\n",
        "\n",
        "The notebook also includes a test gridworld game where the two algorithms are implemented to extract the optimal policy using value iteration and policy iteration.\n",
        "\n",
        "Below is the commented implementation where each section is in a seperate notebook.\n"
      ],
      "metadata": {
        "id": "47TabS3MJEXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "n_Pz4SvuNJYH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid world class representing the dynamics of the grid (enviroment) including the following:**\n",
        "\n",
        "\n",
        "*   Grid size\n",
        "*   Immediate rewards\n",
        "*   Possible actions\n",
        "*   States of the game which are indeed the cells in the grid\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Of6TdXscAsdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class gridworld:\n",
        "\n",
        "  def __init__(self, grid_size):\n",
        "    self.grid_size = grid_size\n",
        "    rewards = -(np.ones((grid_size, grid_size)))\n",
        "    rewards[0,2] = 10\n",
        "    self.rewards = rewards\n",
        "    self.actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
        "    self.action_prob = {\"UP\": (0.8, 0.1, 0.1), \"DOWN\": (0.8, 0.1, 0.1),\n",
        "               \"LEFT\": (0.8, 0.1, 0.1), \"RIGHT\": (0.8, 0.1, 0.1)}\n",
        "\n",
        "  def next_state(self, state, action):\n",
        "    x, y = state\n",
        "    if action == 'UP':\n",
        "        return [(x-1, y), (x, y-1), (x, y+1)]\n",
        "    elif action == 'DOWN':\n",
        "        return [(x+1, y), (x, y-1), (x, y+1)]\n",
        "    elif action == 'LEFT':\n",
        "        return [(x, y-1), (x-1, y), (x+1, y)]\n",
        "    elif action == 'RIGHT':\n",
        "        return [(x, y+1), (x-1, y), (x+1, y)]\n",
        "    return [state, state, state]\n",
        "\n",
        "  def is_valid(self, state):\n",
        "      x,y = state\n",
        "      return 0 <= x < self.grid_size and 0 <= y < self.grid_size\n",
        "\n",
        "  def get_terminal_states(self):\n",
        "    return [(0,0), (0,2)]\n",
        "\n",
        "  def is_terminal_state(self, state):\n",
        "    x,y = state\n",
        "    return (x == 0 and y == 2) or (x == 0 and y == 0)"
      ],
      "metadata": {
        "id": "lPCl8oWkkt-A"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section, the class agent algorithms is implemented including the following:**\n",
        "\n",
        "\n",
        "*   Value Iteration: finds the optimal value function for each state\n",
        "*   Policy Extraction: finds the optimal policy based on the output of the value iteration.\n",
        "*   Policy Iteration\n",
        "\n"
      ],
      "metadata": {
        "id": "bvHweWbjBnJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class agent_algorithms:\n",
        "  def __init__(self, grid: gridworld):\n",
        "    self.grid = grid\n",
        "    self.discount_factor = 0.99 # takes the future highly in consideration.\n",
        "\n",
        "\n",
        "  def value_iteration(self, reward):\n",
        "    state_values = np.zeros((3,3)) #initializng the value function to zero.\n",
        "    state_values[0,0] = reward\n",
        "    state_values[0,2] = 10\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for row in range(0,3):\n",
        "        for col in range(0,3):\n",
        "          max_value = float(\"-inf\")\n",
        "          if (row,col) in self.grid.get_terminal_states():\n",
        "            continue\n",
        "          for action in self.grid.actions:\n",
        "            value = 0\n",
        "            for prob, new_state in zip(self.grid.action_prob[action], self.grid.next_state((row,col), action)):\n",
        "              x1,y1 = new_state\n",
        "              if self.grid.is_valid(new_state):\n",
        "                value += prob * self.discount_factor * state_values[x1,y1]\n",
        "              else:\n",
        "                value += prob * self.discount_factor * state_values[row,col]\n",
        "              value += self.grid.rewards[row,col]\n",
        "            if value > max_value:\n",
        "              state_values[row,col] = value\n",
        "              delta = max(delta, abs(value - state_values[row, col]))\n",
        "      if delta < 1e-4:\n",
        "            break\n",
        "    return state_values\n",
        "\n",
        "  def extract_policy(self, state_values):\n",
        "    policy = np.empty((3, 3), dtype=str)\n",
        "    policy[0,0] = '-'\n",
        "    policy[0,2] = '-'\n",
        "    for row in range(3):\n",
        "        for col in range(3):\n",
        "            max_value = float(\"-inf\")\n",
        "            if (row,col) in self.grid.get_terminal_states():\n",
        "              continue\n",
        "            best_action = None\n",
        "            for action in self.grid.actions:\n",
        "                value = 0\n",
        "                for prob, new_state in zip(self.grid.action_prob[action], self.grid.next_state((row,col), action)):\n",
        "                    x1, y1 = new_state\n",
        "                    if self.grid.is_valid(new_state):\n",
        "                        value += prob * (self.discount_factor * state_values[x1, y1])\n",
        "                    else:\n",
        "                        value += prob * (self.discount_factor * state_values[row, col])\n",
        "                    value+= self.grid.rewards[row, col]\n",
        "                if value > max_value:\n",
        "                    max_value = value\n",
        "                    best_action = action\n",
        "            policy[row, col] = best_action\n",
        "    return policy"
      ],
      "metadata": {
        "id": "Ub0XCnVDQLtd"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section**, The test cases are implemented using different values for the variable reward r and a discount factor = 0.99 (The future is highly accounted in the calculation)"
      ],
      "metadata": {
        "id": "ZIIhGQVZx8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_list = [100,3,0,-3]\n",
        "grid = gridworld(grid_size= 3)\n",
        "for i in range(4):\n",
        "  grid.rewards[0,0] = reward_list[i]\n",
        "  agent = agent_algorithms(grid)\n",
        "  print(f\"State values when r = {reward_list[i]}\")\n",
        "  result = agent.value_iteration(reward_list[i])\n",
        "  policy = agent.extract_policy(result)\n",
        "  print(result)\n",
        "  print()\n",
        "  print(policy)\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OURKWnsEx9gq",
        "outputId": "0ccf9aa6-16ba-42ed-9b68-a6157525a078"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State values when r = 100\n",
            "[[100.          12.54112989  10.        ]\n",
            " [  6.9         -1.75842814  -4.70491867]\n",
            " [ -2.61718107  -3.76157803 -10.06363525]]\n",
            "\n",
            "[['-' 'L' '-']\n",
            " ['U' 'U' 'U']\n",
            " ['U' 'L' 'L']]\n",
            "\n",
            "\n",
            "State values when r = 3\n",
            "[[  3.           4.84143489  10.        ]\n",
            " [ -2.703       -2.52069795  -5.18306308]\n",
            " [ -4.25975138  -3.98084676 -10.34126376]]\n",
            "\n",
            "[['-' 'R' '-']\n",
            " ['U' 'U' 'U']\n",
            " ['U' 'U' 'L']]\n",
            "\n",
            "\n",
            "State values when r = 0\n",
            "[[  0.           4.60329999  10.        ]\n",
            " [ -3.          -2.5442733   -5.19785105]\n",
            " [ -4.31055252  -3.98762826 -10.34985021]]\n",
            "\n",
            "[['-' 'R' '-']\n",
            " ['U' 'U' 'U']\n",
            " ['U' 'U' 'L']]\n",
            "\n",
            "\n",
            "State values when r = -3\n",
            "[[ -3.           4.36516509  10.        ]\n",
            " [ -3.297       -2.56784866  -5.21263902]\n",
            " [ -4.36135366  -3.99440977 -10.35843665]]\n",
            "\n",
            "[['-' 'R' '-']\n",
            " ['R' 'U' 'U']\n",
            " ['U' 'U' 'L']]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGDyTjGFJ7CG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}