{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook includes implementations of the following algorithms:**\n",
        "\n",
        "\n",
        "*   Value Iteration Algorithm\n",
        "*   Policy Iteration Algorithm\n",
        "\n",
        "The notebook also includes a test gridworld game where the two algorithms are implemented to extract the optimal policy using value iteration and policy iteration.\n",
        "\n",
        "Below is the commented implementation where each section is in a seperate notebook.\n"
      ],
      "metadata": {
        "id": "47TabS3MJEXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "n_Pz4SvuNJYH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid world class representing the dynamics of the grid (enviroment) including the following:**\n",
        "\n",
        "\n",
        "*   Grid size\n",
        "*   Immediate rewards\n",
        "*   Possible actions\n",
        "*   States of the game which are indeed the cells in the grid\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Of6TdXscAsdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class gridworld:\n",
        "\n",
        "  def __init__(self, grid_size):\n",
        "    self.grid_size = grid_size\n",
        "    rewards = -(np.ones((grid_size, grid_size)))\n",
        "    rewards[0,2] = 10\n",
        "    self.rewards = rewards\n",
        "    self.actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
        "    self.action_prob = {\"UP\": (0.8, 0.1, 0.1), \"DOWN\": (0.8, 0.1, 0.1),\n",
        "               \"LEFT\": (0.8, 0.1, 0.1), \"RIGHT\": (0.8, 0.1, 0.1)}\n",
        "\n",
        "  def next_state(self, state, action):\n",
        "    x, y = state\n",
        "    if action == 'UP':\n",
        "        return [(x-1, y), (x, y-1), (x, y+1)]\n",
        "    elif action == 'DOWN':\n",
        "        return [(x+1, y), (x, y-1), (x, y+1)]\n",
        "    elif action == 'LEFT':\n",
        "        return [(x, y-1), (x-1, y), (x+1, y)]\n",
        "    elif action == 'RIGHT':\n",
        "        return [(x, y+1), (x-1, y), (x+1, y)]\n",
        "    return [state, state, state]\n",
        "\n",
        "  def is_valid(self, state):\n",
        "      x,y = state\n",
        "      return 0 <= x < self.grid_size and 0 <= y < self.grid_size"
      ],
      "metadata": {
        "id": "lPCl8oWkkt-A"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section, the class agent algorithms is implemented including the following:**\n",
        "\n",
        "\n",
        "*   Value Iteration: finds the optimal value function for each state\n",
        "*   Policy Extraction: finds the optimal policy based on the output of the value iteration.\n",
        "*   Policy Iteration\n",
        "\n"
      ],
      "metadata": {
        "id": "bvHweWbjBnJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class agent_algorithms:\n",
        "  def __init__(self, grid: gridworld):\n",
        "    self.grid = grid\n",
        "    self.discount_factor = 0.99 # takes the future highly in consideration.\n",
        "\n",
        "\n",
        "  def value_iteration(self):\n",
        "    state_values = np.zeros((3,3)) #initializng the value function to zero.\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for row in range(0,3):\n",
        "        for col in range(0,3):\n",
        "          max_value = float(\"-inf\")\n",
        "          for action in self.grid.actions:\n",
        "            value = 0\n",
        "            for prob, new_state in zip(self.grid.action_prob[action], self.grid.next_state((row,col), action)):\n",
        "              x1,y1 = new_state\n",
        "              if self.grid.is_valid(new_state):\n",
        "                value += prob * self.discount_factor * state_values[x1,y1]\n",
        "              else:\n",
        "                value += prob * self.discount_factor * state_values[row,col]\n",
        "              value += self.grid.rewards[row,col]\n",
        "            if value > max_value:\n",
        "              state_values[row,col] = value\n",
        "              delta = max(delta, abs(value - state_values[row, col]))\n",
        "      if delta < 1e-4:\n",
        "            break\n",
        "    return state_values\n",
        "\n",
        "  def extract_policy(self, state_values):\n",
        "    policy = np.empty((3, 3), dtype=str)\n",
        "    for row in range(3):\n",
        "        for col in range(3):\n",
        "            max_value = float(\"-inf\")\n",
        "            best_action = None\n",
        "            for action in self.grid.actions:\n",
        "                value = 0\n",
        "                for prob, new_state in zip(self.grid.action_prob[action], self.grid.next_state((row,col), action)):\n",
        "                    x1, y1 = new_state\n",
        "                    if self.grid.is_valid(new_state):\n",
        "                        value += prob * (self.discount_factor * state_values[x1, y1])\n",
        "                    else:\n",
        "                        value += prob * (self.discount_factor * state_values[row, col])\n",
        "                    value+= self.grid.rewards[row, col]\n",
        "                if value > max_value:\n",
        "                    max_value = value\n",
        "                    best_action = action\n",
        "            policy[row, col] = best_action\n",
        "    return policy\n",
        "\n",
        "  def policy_iteration(self):\n",
        "        state_values = np.zeros((3, 3))  # initializng the value function to zero.\n",
        "        policy = np.random.choice(self.grid.actions, size=(3, 3))  # random initial policy\n",
        "        while True:\n",
        "            # Policy Evaluation\n",
        "            while True:\n",
        "                delta = 0\n",
        "                for row in range(0, 3):\n",
        "                    for col in range(0, 3):\n",
        "                        action = policy[row, col]\n",
        "                        value = 0\n",
        "                        for prob, new_state in zip(self.grid.action_prob[action], self.grid.next_state((row, col), action)):\n",
        "                            x1, y1 = new_state\n",
        "                            if self.grid.is_valid(new_state):\n",
        "                                value += prob * (self.discount_factor * state_values[x1, y1])\n",
        "                            else:\n",
        "                                value += prob * (self.discount_factor * state_values[row, col])\n",
        "                            value += self.grid.rewards[row, col]\n",
        "                        delta = max(delta, abs(value - state_values[row, col]))\n",
        "                        state_values[row, col] = value\n",
        "                if delta < 1e-4:\n",
        "                    break\n",
        "\n",
        "            # Policy Improvement\n",
        "            policy_stable = True\n",
        "            for row in range(0, 3):\n",
        "                for col in range(0, 3):\n",
        "                    old_action = policy[row, col]\n",
        "                    max_value = float(\"-inf\")\n",
        "                    best_action = None\n",
        "                    for action in self.grid.actions:\n",
        "                        value = 0\n",
        "                        for prob, new_state in zip(self.grid.action_prob[action], self.grid.next_state((row, col), action)):\n",
        "                            x1, y1 = new_state\n",
        "                            if self.grid.is_valid(new_state):\n",
        "                                value += prob * (self.discount_factor * state_values[x1, y1])\n",
        "                            else:\n",
        "                                value += prob * (self.discount_factor * state_values[row, col])\n",
        "                            value += self.grid.rewards[row, col]\n",
        "                        if value > max_value:\n",
        "                            max_value = value\n",
        "                            best_action = action\n",
        "                    policy[row, col] = best_action\n",
        "                    if old_action != best_action:\n",
        "                        policy_stable = False\n",
        "            if policy_stable:\n",
        "                break\n",
        "\n",
        "        return state_values, policy"
      ],
      "metadata": {
        "id": "Ub0XCnVDQLtd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this section**, The test cases are implemented using different values for the variable reward r and a discount factor = 0.99 (The future is highly accounted in the calculation)"
      ],
      "metadata": {
        "id": "ZIIhGQVZx8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_list = [100,3,0,-3]\n",
        "grid = gridworld(grid_size= 3)\n",
        "for i in range(4):\n",
        "  grid.rewards[0,0] = reward_list[i]\n",
        "  agent = agent_algorithms(grid)\n",
        "  print(f\"State values when r = {reward_list[i]}\")\n",
        "  result = agent.value_iteration()\n",
        "  policy = agent.extract_policy(result)\n",
        "  print(result)\n",
        "  print()\n",
        "  print(policy)\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OURKWnsEx9gq",
        "outputId": "0e3dd7a9-744b-4842-a736-2899afbb2c51"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State values when r = 100\n",
            "[[358.7825073   25.15314214  77.62951928]\n",
            " [ 32.51946822  -0.50983893   8.07629415]\n",
            " [  1.76496803  -3.26683906  -6.51799654]]\n",
            "\n",
            "[['L' 'L' 'U']\n",
            " ['U' 'L' 'U']\n",
            " ['U' 'L' 'U']]\n",
            "\n",
            "\n",
            "State values when r = 3\n",
            "[[10.76347522 -2.47201665 57.87009329]\n",
            " [-1.93441595 -3.24472965  2.85531541]\n",
            " [-4.1282869  -4.05353689 -8.38854082]]\n",
            "\n",
            "[['L' 'R' 'R']\n",
            " ['U' 'R' 'U']\n",
            " ['U' 'U' 'U']]\n",
            "\n",
            "\n",
            "State values when r = 0\n",
            "[[ 0.         -3.326403   57.25897703]\n",
            " [-3.         -3.3293139   2.69384184]\n",
            " [-4.31055252 -4.07786775 -8.4463927 ]]\n",
            "\n",
            "[['L' 'R' 'R']\n",
            " ['U' 'R' 'U']\n",
            " ['U' 'U' 'U']]\n",
            "\n",
            "\n",
            "State values when r = -3\n",
            "[[-10.76347522  -4.18078935  56.64786076]\n",
            " [ -4.06558405  -3.41389815   2.53236827]\n",
            " [ -4.49281814  -4.10219861  -8.50424459]]\n",
            "\n",
            "[['D' 'R' 'R']\n",
            " ['R' 'R' 'U']\n",
            " ['U' 'U' 'U']]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This section** tests the **Policy Iteration** algorithm. It iterates over a list of reward values, sets each value as the reward in the top-left corner of the grid, computes the optimal policy and state values using Policy Iteration, and prints the results, showing how varying rewards affect the learned optimal policies and expected cumulative rewards."
      ],
      "metadata": {
        "id": "h4_Kzlk_N-J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the Policy Iteration algorithm\n",
        "reward_list = [100, 3, 0, -3]\n",
        "grid = gridworld(grid_size=3)\n",
        "\n",
        "for i in range(4):\n",
        "    grid.rewards[0, 0] = reward_list[i]\n",
        "    agent = agent_algorithms(grid)\n",
        "    print(f\"State values and policy when r = {reward_list[i]}:\")\n",
        "    result, policy = agent.policy_iteration()\n",
        "    print(result)\n",
        "    print()\n",
        "    print(policy)\n",
        "    print()"
      ],
      "metadata": {
        "id": "l_-ngxGwNdoc",
        "outputId": "5d26d87c-cf76-49f2-86ae-493645a6bebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State values and policy when r = 100:\n",
            "[[26319.52754445 25947.7635734  25626.09615308]\n",
            " [25947.7635734  25624.94123665 25338.22122842]\n",
            " [25585.80882725 25304.89920855 25050.00690979]]\n",
            "\n",
            "[['UP' 'LEFT' 'LEFT']\n",
            " ['UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP']]\n",
            "\n",
            "State values and policy when r = 3:\n",
            "[[2535.02473987 2559.73349111 2600.12416359]\n",
            " [2502.50966115 2525.47099703 2559.73358189]\n",
            " [2470.16757945 2491.24601732 2520.46874987]]\n",
            "\n",
            "[['RIGHT' 'RIGHT' 'RIGHT']\n",
            " ['UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP']]\n",
            "\n",
            "State values and policy when r = 0:\n",
            "[[2522.82081172 2558.58354384 2599.0796592 ]\n",
            " [2491.55032642 2523.36141987 2558.58364149]\n",
            " [2460.22803927 2488.46084859 2519.15189624]]\n",
            "\n",
            "[['RIGHT' 'RIGHT' 'RIGHT']\n",
            " ['UP' 'UP' 'UP']\n",
            " ['UP' 'UP' 'UP']]\n",
            "\n",
            "State values and policy when r = -3:\n",
            "[[2511.98041709 2558.20121661 2598.73247959]\n",
            " [2486.8594086  2522.65920518 2558.20130559]\n",
            " [2456.84177819 2487.89828187 2518.75400136]]\n",
            "\n",
            "[['RIGHT' 'RIGHT' 'RIGHT']\n",
            " ['RIGHT' 'RIGHT' 'UP']\n",
            " ['RIGHT' 'RIGHT' 'UP']]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}